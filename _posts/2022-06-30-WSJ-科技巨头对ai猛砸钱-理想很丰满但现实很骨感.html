---
layout: post
title: 科技巨头对AI猛砸钱，理想很丰满但现实很骨感
date: 2022-06-30 06:20:00.000000000 +00:00
link: https://cn.wsj.com/articles/%E7%A7%91%E6%8A%80%E5%B7%A8%E5%A4%B4%E5%AF%B9ai%E7%8C%9B%E7%A0%B8%E9%92%B1-%E7%90%86%E6%83%B3%E5%BE%88%E4%B8%B0%E6%BB%A1%E4%BD%86%E7%8E%B0%E5%AE%9E%E5%BE%88%E9%AA%A8%E6%84%9F-11656570608
categories: wsj
---

<p>多年来，企业一直大谈人工智能的潜力，研究人员说，现在是时候调整预期了。</p>
      <p>随着最近这项技术的飞跃，多家公司已经开发出更多的系统，可以产生类似人类的对话、诗歌和图像。然而，人工智能伦理学家和研究人员警告说，一些企业正在夸大这些能力，他们说，这种炒作正在催生广泛的误解，并扭曲了决策者对这种技术的能力和缺陷的看法。</p>
      <p>总部位于西雅图的非营利性研究机构艾伦人工智能研究所(Allen Institute for Artificial Intelligence)的首席执行官Oren Etzioni称：“我们失去了平衡。”</p>
       <p>他和其他研究人员称，这种不平衡有助于解释本月早些时候Alphabet Inc. (GOOG) 旗下谷歌一位工程师的观点为何让许多人动摇了。当时谷歌的一名工程师基于自己的宗教信仰指出，该公司的一个人工智能系统应该被认为是有知觉的。</p>
      <p>这位工程师说，这个聊天机器人<a href="https://cn.wsj.com/articles/CN-TEC-20220613110347" target="_blank" class="icon none">实际上已经成为一个人</a>，有权决定是否同意对它进行试验。谷歌暂停了他的工作，且不认同他的说法，称公司的伦理学家和技术专家已经研究了这种可能性，不予认可。</p>
      <p>研究人员称，认为人工智能变得拥有意识，或者可能拥有意识的想法，在整体科学界仍然处于边缘地位。</p>
      <p>从现实层面看，人工智能涵盖的一系列技术在很大程度上仍对一些不起眼的后台后勤工作有帮助，比如处理来自用户的数据，以便更精准地向他们投放广告及内容并推荐产品。</p>
      <p>过去十年里，谷歌、Facebook母公司Meta Platforms Inc. (META)和亚马逊公司(Amazon.com Inc., AMZN)等公司已投入巨资发展这些能力，为的是助力公司增长并提振利润。</p>
      <div class="media-object scope-
          
        ">
      
      
          <div class="media-object-video">
      <div class="wsj-media-summary clearfix">
    <div class="strap-container">
         <h4 class="strap" itemprop="description">
        
        </h4>
  </div>
  </div>
  <iframe width="400" height="225" src="https://video-api.wsj.com/api-video/player/v3/iframe.html?guid=E98D36B8-2386-4E15-91FE-E842716652A1&amp;height=225&amp;width=400&amp;plid=video_amp&amp;chainVideos=true&amp;parentUrl=https%3A%2F%2Fcn.wsj.com%2Famp%2Farticles%2F%25E7%25A7%2591%25E6%258A%2580%25E5%25B7%25A8%25E5%25A4%25B4%25E5%25AF%25B9ai%25E7%258C%259B%25E7%25A0%25B8%25E9%2592%25B1-%25E7%2590%2586%25E6%2583%25B3%25E5%25BE%2588%25E4%25B8%25B0%25E6%25BB%25A1%25E4%25BD%2586%25E7%258E%25B0%25E5%25AE%259E%25E5%25BE%2588%25E9%25AA%25A8%25E6%2584%259F-11656570608&amp;a=a" referrerpolicy="no-referrer"></iframe>
      <p class="imageCaption">
        逐鹿元宇宙之战正在升温，如同美国的Meta和微软，中国的百度和腾讯等公司也正大举投资这一新兴领域。但中国有着更严格的技术法规，游戏和加密货币产业已经受到影响，这意味着，中国未来虚拟世界的用户体验将与美国的有着很大不同。封面图片制作：Michelle Inez Simon
        <span class="imageCredit">
        
        WSJ S Chinese
        </span>
      </p>
</div>

      
      
      
      
      
      
      
      
      
      
      
      
      
      
         
      
      
      
      </div>
      <p>以谷歌为例，该公司运用人工智能来完善对复杂搜索提示的解析，协助其给出相关的广告和网络搜索结果。</p>
      <p>一些初创企业也萌生了更宏大的雄心。其中一家名为OpenAI的公司从特斯拉(Tesla Inc., TSLA)首席执行官马斯克(Elon Musk)和微软(Microsoft Corp., MSFT)等投资方手中筹得了数十亿美元资金，要实现所谓的通用人工智能(AGI)，该系统将具备与人类同等的智慧甚至全面超越人类。一些研究人员认为，即便AGI并非高不可攀，要实现它也需要几十年时间。</p>
      <p>这些公司你追我赶、互不相让的竞争氛围推动了AI技术的快速发展，催生出越来越多广受好评的演示，激发了公众的想象力及对AI技术的关注热情。</p>
      <div class="media-object scope-
          inline
">
      
          <figure class="media-object-image img-inline">
    
      
      <div class="image-container responsive-media">
        <img width="700" height="466" src="https://images.wsj.net/im-566037?width=700&amp;height=466" alt="" referrerpolicy="no-referrer">
      </div>
    
      <figcaption class="imageCaption">
        <p class="imageCaptionContent">谷歌AI伦理团队的联合负责人Margaret Mitchell在写了一篇批评性的论文后被解雇；谷歌表示，解雇Mitchell的原因是她与公司以外的人分享内部文件。</p>
    <p class="imageCredit"> 图片来源：CHONA KASINGER/BLOOMBERG NEWS</p>
  </figcaption>
</figure>

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
         
      
      
      
      </div>
      <p>最近几周，OpenAI的DALL-E系统在社交媒体上吸引网民创作出许多梗图。此系统可根据用户输入的文本生成艺术图像，比如“土星轨道上的麦当劳(McDonald's)”或“铁人三项赛中身着运动装备的熊”。</p>
      <p>谷歌也紧随其后，亮出了自家基于文本输入的艺术创作系统。</p>
      <p>虽然这些创意可能令人惊叹，但越来越多的专家警告说，公司没有适当地控制炒作。</p>
      <p>Margaret Mitchell曾是谷歌AI伦理团队的联合负责人，她在写了一篇<a href="https://cn.wsj.com/articles/CN-TEC-20201204120709" target="_blank" class="icon none">批评谷歌系统的论文</a>后被公司解雇。她说，这家搜索巨头向股东宣传的卖点之一是，它在人工智能领域是全球做得最好的。</p>
      <p>Mitchell目前在一家名为Hugging Face的AI初创公司工作，她和谷歌AI伦理团队的另一位联合负责人Timnit Gebru是最早提醒人们注意该技术危险性的人之一。Gebru也被谷歌解雇。</p>
      <p>在供职谷歌期间写的最后一篇论文中，她们称这些技术有时会造成伤害，AI的类人能力意味着有和人类一样的失败可能。她们列举的例子包括，Facebook的AI系统将阿拉伯语的“早上好”误译为英语的“伤害他们”和希伯来语的“攻击他们”，导致以色列警方逮捕了发布该问候语的巴勒斯坦男子，后来警方意识到了自己的失误。</p>
      <div class="media-object scope-
          inline
">
      
          <figure class="media-object-image img-inline">
    
      
      <div class="image-container responsive-media">
        <img width="700" height="466" src="https://images.wsj.net/im-566041?width=700&amp;height=466" alt="" referrerpolicy="no-referrer">
      </div>
    
      <figcaption class="imageCaption">
        <p class="imageCaptionContent">著名的AI伦理学家、谷歌AI伦理团队的另一位联合负责人Timnit Gebru也被解雇。</p>
    <p class="imageCredit"> 图片来源：JEFF CHIU/ASSOCIATED PRESS</p>
  </figcaption>
</figure>

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
         
      
      
      
      </div>
      <p>去年出版的<a href="https://cn.wsj.com/articles/CN-ATG-20211012155245" target="_blank" class="icon none">《Facebook文件》系列报道</a>援引的内部文件也显示，Facebook的系统<a href="https://cn.wsj.com/articles/CN-TEC-20211108080619" target="_blank" class="icon none">未能持续一贯地识别</a>第一人称射击视频和种族主义言论，只删除了违反该公司规定的内容中的一小部分。《华尔街日报》(The Wall Street Journal) 看到了这些内部文件。</p>
      <p>Facebook表示，其人工智能的改进大大减少了仇恨言论和其他违反其规定的内容。</p>
      <p>谷歌表示，解雇Mitchell的原因是她与公司以外的人分享内部文件。谷歌的人工智能主管告诉员工称，Gebru的工作不够严谨。</p>
      <p>上述解聘事件在科技行业造成振动，引发谷歌内外数以千计的人谴责他们在一份请愿书中所称的“前所未有的研究审查制”。谷歌首席执行官皮查伊(Sundar Pichai)表示，他将努力在这些问题上恢复人们的信任，并承诺把研究人工智能伦理的人数增加一倍。</p>
      <p>感觉和现实之间存在差距并不是什么新鲜事。Etzioni和其他人提到了有关Watson的营销，Watson是International Business Machines Corp. (IBM)开发的人工智能业务，在智力竞赛节目《危险边缘》(Jeopardy)中打败人类后变得广为人知。在进行了10年和数以十亿美元计的投资后，IBM去年表示正探索出售Watson Health的可能性，该部门的主营产品本应用于帮助医生诊断和治疗癌症。</p>
      <p>人工智能现在无处不在，而且涉及更多公司，因此变得更举足轻重。这些公司开发的软件包括电子邮件、搜索引擎、动态消息、语音助手，已渗透至大众数字生活的各个角落。</p>
      <p>在上述工程师发表最近的说法之后，谷歌反驳了其聊天机器人具有感知力的说法。</p>
      <p>谷歌的聊天机器人和其他对话工具“可以大聊任何虚幻的话题”，该公司发言人Brian Gabriel表示。“如果你问成为一只冰激凌恐龙是什么感觉，它们可以生成与融化、咆哮等有关的文本。”他说，这并不等同于有意识。</p>
      <p>现已被停职的那位工程师Blake Lemoine在接受采访时称，他曾汇集了数以百页计来自聊天机器人LaMDA相关受控实验的对话以支持自己的研究，他确切介绍了谷歌程序的内部运作情况。</p>
      <div class="media-object scope-
          wrap
">
      
          <figure class="media-object-image img-wrap">
    
      
      <div class="image-container responsive-media">
        <img width="639" height="852" src="https://images.wsj.net/im-566054?width=639&amp;height=852" alt="" referrerpolicy="no-referrer">
      </div>
    
      <figcaption class="imageCaption">
        <p class="imageCaptionContent">谷歌停职了Blake Lemoine，并否认了他关于其一个人工智能系统应被视为具有感知力的说法。</p>
    <p class="imageCredit"> 图片来源：MARTIN KLIMEK/THE WASHINGTON POST/GETTY IMAGES</p>
  </figcaption>
</figure>

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
         
      
      
      
      </div>
      <p>“这并不是夸大该系统的本质，”Lemoine说。“我正尽可能小心、精确地传递讯息，说清哪里有不确定性，哪里没有。”</p>
      <p>Lemoine自称是一个神秘主义者，融合了基督教以及冥想等其他精神实践的元素。他已表示，当他说LaMDA有意识的时候，是在以宗教身份说话。</p>
      <p>布朗大学(Brown University)研究人工智能政策的计算机科学博士生Elizabeth Kumar称，认知差距已经在政策文件中悄然显现。最近的地方、联邦和国际规定以及监管提案，纷纷寻求应对人工智能系统可能通过歧视、操纵或其他方式造成伤害的问题，这些都是基于系统具有高度能力的假设。她说，一种更有可能发生的情况在很大程度上被忽略了，即此类人工智能系统因失灵而造成伤害。</p>
      <p>Etzioni也是拜登(Joe Biden)政府的国家人工智能研究资源工作组(National Artificial Intelligence Research Resource Task Force)的成员，他表示，政策制定者往往难以把握这些问题。他说：“我可以告诉你，从我与他们中的一些人的交流来看，他们的出发点是好的，也问出了一些好问题，但他们并非对情况了如指掌。”</p>
